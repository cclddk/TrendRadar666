# coding=utf-8

import json
import os
import random
import re
import time
import webbrowser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from email.utils import formataddr, formatdate, make_msgid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

import pytz
import requests
import yaml


VERSION = "3.0.6"  # ä¿®æ”¹ç‰ˆæœ¬å·ä»¥ç¤ºåŒºåˆ«


# === SMTPé‚®ä»¶é…ç½® ===
SMTP_CONFIGS = {
    # Gmailï¼ˆä½¿ç”¨ STARTTLSï¼‰
    "gmail.com": {"server": "smtp.gmail.com", "port": 587, "encryption": "TLS"},
    # QQé‚®ç®±ï¼ˆä½¿ç”¨ SSLï¼Œæ›´ç¨³å®šï¼‰
    "qq.com": {"server": "smtp.qq.com", "port": 465, "encryption": "SSL"},
    # Outlookï¼ˆä½¿ç”¨ STARTTLSï¼‰
    "outlook.com": {
        "server": "smtp-mail.outlook.com",
        "port": 587,
        "encryption": "TLS",
    },
    "hotmail.com": {
        "server": "smtp-mail.outlook.com",
        "port": 587,
        "encryption": "TLS",
    },
    "live.com": {"server": "smtp-mail.outlook.com", "port": 587, "encryption": "TLS"},
    # ç½‘æ˜“é‚®ç®±ï¼ˆä½¿ç”¨ SSLï¼Œæ›´ç¨³å®šï¼‰
    "163.com": {"server": "smtp.163.com", "port": 465, "encryption": "SSL"},
    "126.com": {"server": "smtp.126.com", "port": 465, "encryption": "SSL"},
    # æ–°æµªé‚®ç®±ï¼ˆä½¿ç”¨ SSLï¼‰
    "sina.com": {"server": "smtp.sina.com", "port": 465, "encryption": "SSL"},
    # æœç‹é‚®ç®±ï¼ˆä½¿ç”¨ SSLï¼‰
    "sohu.com": {"server": "smtp.sohu.com", "port": 465, "encryption": "SSL"},
}


# === é…ç½®ç®¡ç† ===
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.environ.get("CONFIG_PATH", "config/config.yaml")

    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")

    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)

    print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")

    # æ„å»ºé…ç½®
    config = {
        "VERSION_CHECK_URL": config_data["app"]["version_check_url"],
        "SHOW_VERSION_UPDATE": config_data["app"]["show_version_update"],
        "REQUEST_INTERVAL": config_data["crawler"]["request_interval"],
        "REPORT_MODE": os.environ.get("REPORT_MODE", "").strip()
        or config_data["report"]["mode"],
        "RANK_THRESHOLD": config_data["report"]["rank_threshold"],
        "USE_PROXY": config_data["crawler"]["use_proxy"],
        "DEFAULT_PROXY": config_data["crawler"]["default_proxy"],
        "ENABLE_CRAWLER": os.environ.get("ENABLE_CRAWLER", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_CRAWLER", "").strip()
        else config_data["crawler"]["enable_crawler"],
        "ENABLE_NOTIFICATION": os.environ.get("ENABLE_NOTIFICATION", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_NOTIFICATION", "").strip()
        else config_data["notification"]["enable_notification"],
        "MESSAGE_BATCH_SIZE": config_data["notification"]["message_batch_size"],
        "DINGTALK_BATCH_SIZE": config_data["notification"].get(
            "dingtalk_batch_size", 20000
        ),
        "FEISHU_BATCH_SIZE": config_data["notification"].get("feishu_batch_size", 29000),
        "BATCH_SEND_INTERVAL": config_data["notification"]["batch_send_interval"],
        "FEISHU_MESSAGE_SEPARATOR": config_data["notification"][
            "feishu_message_separator"
        ],
        "PUSH_WINDOW": {
            "ENABLED": os.environ.get("PUSH_WINDOW_ENABLED", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ENABLED", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("enabled", False),
            "TIME_RANGE": {
                "START": os.environ.get("PUSH_WINDOW_START", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("start", "08:00"),
                "END": os.environ.get("PUSH_WINDOW_END", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("end", "22:00"),
            },
            "ONCE_PER_DAY": os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("once_per_day", True),
            "RECORD_RETENTION_DAYS": int(
                os.environ.get("PUSH_WINDOW_RETENTION_DAYS", "").strip() or "0"
            )
            or config_data["notification"]
            .get("push_window", {})
            .get("push_record_retention_days", 7),
        },
        "WEIGHT_CONFIG": {
            "RANK_WEIGHT": config_data["weight"]["rank_weight"],
            "FREQUENCY_WEIGHT": config_data["weight"]["frequency_weight"],
            "HOTNESS_WEIGHT": config_data["weight"]["hotness_weight"],
        },
        "PLATFORMS": config_data["platforms"],
    }

    # é€šçŸ¥æ¸ é“é…ç½®ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰
    notification = config_data.get("notification", {})
    webhooks = notification.get("webhooks", {})

    config["FEISHU_WEBHOOK_URL"] = os.environ.get(
        "FEISHU_WEBHOOK_URL", ""
    ).strip() or webhooks.get("feishu_url", "")
    config["DINGTALK_WEBHOOK_URL"] = os.environ.get(
        "DINGTALK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("dingtalk_url", "")
    config["WEWORK_WEBHOOK_URL"] = os.environ.get(
        "WEWORK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("wework_url", "")
    config["TELEGRAM_BOT_TOKEN"] = os.environ.get(
        "TELEGRAM_BOT_TOKEN", ""
    ).strip() or webhooks.get("telegram_bot_token", "")
    config["TELEGRAM_CHAT_ID"] = os.environ.get(
        "TELEGRAM_CHAT_ID", ""
    ).strip() or webhooks.get("telegram_chat_id", "")

    # é‚®ä»¶é…ç½®
    config["EMAIL_FROM"] = os.environ.get("EMAIL_FROM", "").strip() or webhooks.get(
        "email_from", ""
    )
    config["EMAIL_PASSWORD"] = os.environ.get(
        "EMAIL_PASSWORD", ""
    ).strip() or webhooks.get("email_password", "")
    config["EMAIL_TO"] = os.environ.get("EMAIL_TO", "").strip() or webhooks.get(
        "email_to", ""
    )
    config["EMAIL_SMTP_SERVER"] = os.environ.get(
        "EMAIL_SMTP_SERVER", ""
    ).strip() or webhooks.get("email_smtp_server", "")
    config["EMAIL_SMTP_PORT"] = os.environ.get(
        "EMAIL_SMTP_PORT", ""
    ).strip() or webhooks.get("email_smtp_port", "")

    # ntfyé…ç½®
    config["NTFY_SERVER_URL"] = os.environ.get(
        "NTFY_SERVER_URL", "https://ntfy.sh"
    ).strip() or webhooks.get("ntfy_server_url", "https://ntfy.sh")
    config["NTFY_TOPIC"] = os.environ.get("NTFY_TOPIC", "").strip() or webhooks.get(
        "ntfy_topic", ""
    )
    config["NTFY_TOKEN"] = os.environ.get("NTFY_TOKEN", "").strip() or webhooks.get(
        "ntfy_token", ""
    )

    return config


print("æ­£åœ¨åŠ è½½é…ç½®...")
CONFIG = load_config()
print(f"TrendRadar v{VERSION} é…ç½®åŠ è½½å®Œæˆ")
print(f"ç›‘æ§å¹³å°æ•°é‡: {len(CONFIG['PLATFORMS'])}")


# === å·¥å…·å‡½æ•° ===
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(pytz.timezone("Asia/Shanghai"))


def format_date_folder():
    """æ ¼å¼åŒ–æ—¥æœŸæ–‡ä»¶å¤¹"""
    return get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")


def format_time_filename():
    """æ ¼å¼åŒ–æ—¶é—´æ–‡ä»¶å"""
    return get_beijing_time().strftime("%Hæ—¶%Måˆ†")


def clean_title(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    if not isinstance(title, str):
        title = str(title)
    cleaned_title = title.replace("\n", " ").replace("\r", " ")
    cleaned_title = re.sub(r"\s+", " ", cleaned_title)
    cleaned_title = cleaned_title.strip()
    return cleaned_title


def ensure_directory_exists(directory: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(directory).mkdir(parents=True, exist_ok=True)


def get_output_path(subfolder: str, filename: str) -> str:
    """è·å–è¾“å‡ºè·¯å¾„"""
    date_folder = format_date_folder()
    output_dir = Path("output") / date_folder / subfolder
    ensure_directory_exists(str(output_dir))
    return str(output_dir / filename)


def check_version_update(
    current_version: str, version_url: str, proxy_url: Optional[str] = None
) -> Tuple[bool, Optional[str]]:
    """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
    try:
        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/plain, */*",
            "Cache-Control": "no-cache",
        }

        response = requests.get(
            version_url, proxies=proxies, headers=headers, timeout=10
        )
        response.raise_for_status()

        remote_version = response.text.strip()
        print(f"å½“å‰ç‰ˆæœ¬: {current_version}, è¿œç¨‹ç‰ˆæœ¬: {remote_version}")
        return False, None  # ç®€åŒ–ç‰ˆæœ¬æ£€æŸ¥é€»è¾‘

    except Exception as e:
        print(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
        return False, None


def is_first_crawl_today() -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯å½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return True

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    return len(files) <= 1


def html_escape(text: str) -> str:
    """HTMLè½¬ä¹‰"""
    if not isinstance(text, str):
        text = str(text)

    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#x27;")
    )


# === æ¨é€è®°å½•ç®¡ç† (ä¿æŒä¸å˜) ===
class PushRecordManager:
    """æ¨é€è®°å½•ç®¡ç†å™¨"""

    def __init__(self):
        self.record_dir = Path("output") / ".push_records"
        self.ensure_record_dir()
        self.cleanup_old_records()

    def ensure_record_dir(self):
        self.record_dir.mkdir(parents=True, exist_ok=True)

    def get_today_record_file(self) -> Path:
        today = get_beijing_time().strftime("%Y%m%d")
        return self.record_dir / f"push_record_{today}.json"

    def cleanup_old_records(self):
        retention_days = CONFIG["PUSH_WINDOW"]["RECORD_RETENTION_DAYS"]
        current_time = get_beijing_time()
        for record_file in self.record_dir.glob("push_record_*.json"):
            try:
                date_str = record_file.stem.replace("push_record_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d")
                file_date = pytz.timezone("Asia/Shanghai").localize(file_date)
                if (current_time - file_date).days > retention_days:
                    record_file.unlink()
            except Exception:
                pass

    def has_pushed_today(self) -> bool:
        record_file = self.get_today_record_file()
        if not record_file.exists():
            return False
        try:
            with open(record_file, "r", encoding="utf-8") as f:
                record = json.load(f)
            return record.get("pushed", False)
        except Exception:
            return False

    def record_push(self, report_type: str):
        record_file = self.get_today_record_file()
        now = get_beijing_time()
        record = {
            "pushed": True,
            "push_time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "report_type": report_type,
        }
        try:
            with open(record_file, "w", encoding="utf-8") as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜æ¨é€è®°å½•å¤±è´¥: {e}")

    def is_in_time_range(self, start_time: str, end_time: str) -> bool:
        now = get_beijing_time()
        current_time = now.strftime("%H:%M")
    
        def normalize_time(time_str: str) -> str:
            try:
                parts = time_str.strip().split(":")
                return f"{int(parts[0]):02d}:{int(parts[1]):02d}"
            except:
                return time_str
    
        return normalize_time(start_time) <= normalize_time(current_time) <= normalize_time(end_time)


# === æ•°æ®è·å– (ä¿æŒä¸å˜) ===
class DataFetcher:
    """æ•°æ®è·å–å™¨"""
    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(self, id_info: Union[str, Tuple[str, str]], max_retries: int = 2):
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value
        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"
        proxies = {"http": self.proxy_url, "https": self.proxy_url} if self.proxy_url else None
        headers = {"User-Agent": "Mozilla/5.0"}

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(url, proxies=proxies, headers=headers, timeout=10)
                response.raise_for_status()
                data_json = response.json()
                if data_json.get("status") in ["success", "cache"]:
                    return response.text, id_value, alias
                raise ValueError(f"Status: {data_json.get('status')}")
            except Exception as e:
                retries += 1
                time.sleep(random.uniform(1, 3))
        return None, id_value, alias

    def crawl_websites(self, ids_list: List, request_interval: int = 1000):
        results = {}
        id_to_name = {}
        failed_ids = []
        for i, id_info in enumerate(ids_list):
            id_value = id_info[0] if isinstance(id_info, tuple) else id_info
            name = id_info[1] if isinstance(id_info, tuple) else id_value
            id_to_name[id_value] = name
            
            resp, _, _ = self.fetch_data(id_info)
            if resp:
                try:
                    data = json.loads(resp)
                    results[id_value] = {}
                    for idx, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        if title in results[id_value]:
                            results[id_value][title]["ranks"].append(idx)
                        else:
                            results[id_value][title] = {
                                "ranks": [idx],
                                "url": item.get("url", ""),
                                "mobileUrl": item.get("mobileUrl", "")
                            }
                except:
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)
            if i < len(ids_list) - 1:
                time.sleep(request_interval / 1000)
        return results, id_to_name, failed_ids


# === æ•°æ®å¤„ç† ===
def save_titles_to_file(results: Dict, id_to_name: Dict, failed_ids: List) -> str:
    # ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜
    file_path = get_output_path("txt", f"{format_time_filename()}.txt")
    with open(file_path, "w", encoding="utf-8") as f:
        for id_value, title_data in results.items():
            name = id_to_name.get(id_value, id_value)
            f.write(f"{id_value} | {name}\n")
            sorted_titles = []
            for title, info in title_data.items():
                rank = info["ranks"][0] if info["ranks"] else 1
                sorted_titles.append((rank, clean_title(title), info.get("url"), info.get("mobileUrl")))
            sorted_titles.sort(key=lambda x: x[0])
            for rank, t, u, m in sorted_titles:
                line = f"{rank}. {t}"
                if u: line += f" [URL:{u}]"
                if m: line += f" [MOBILE:{m}]"
                f.write(line + "\n")
            f.write("\n")
        if failed_ids:
            f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
            for id_value in failed_ids:
                f.write(f"{id_value}\n")
    return file_path

def load_frequency_words(frequency_file: Optional[str] = None):
    # ä¿æŒåŸæœ‰é€»è¾‘
    if frequency_file is None:
        frequency_file = os.environ.get("FREQUENCY_WORDS_PATH", "config/frequency_words.txt")
    if not Path(frequency_file).exists():
        return [], []
    
    with open(frequency_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    groups = []
    filters = []
    for g_text in content.split("\n\n"):
        if not g_text.strip(): continue
        req, norm = [], []
        for word in g_text.split("\n"):
            w = word.strip()
            if not w: continue
            if w.startswith("!"): filters.append(w[1:])
            elif w.startswith("+"): req.append(w[1:])
            else: norm.append(w)
        if req or norm:
            groups.append({"required": req, "normal": norm, "group_key": " ".join(norm) if norm else " ".join(req)})
    return groups, filters

def parse_file_titles(file_path: Path) -> Tuple[Dict, Dict]:
    # ç®€åŒ–ç‰ˆï¼šå¤ç”¨åŸæœ‰é€»è¾‘ï¼Œä¸èµ˜è¿°
    titles_by_id = {}
    id_to_name = {}
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    for section in content.split("\n\n"):
        if not section.strip() or "å¤±è´¥" in section: continue
        lines = section.strip().split("\n")
        header = lines[0].split(" | ")
        sid = header[0].strip()
        id_to_name[sid] = header[1].strip() if len(header) > 1 else sid
        titles_by_id[sid] = {}
        for line in lines[1:]:
            if not line.strip(): continue
            try:
                parts = line.split(". ", 1)
                rank = int(parts[0]) if parts[0].isdigit() else 1
                rest = parts[1]
                url, mobile = "", ""
                if " [URL:" in rest:
                    rest, url = rest.split(" [URL:", 1)
                    url = url.rstrip("]")
                if " [MOBILE:" in rest:
                    rest, mobile = rest.split(" [MOBILE:", 1)
                    mobile = mobile.rstrip("]")
                titles_by_id[sid][clean_title(rest)] = {"ranks": [rank], "url": url, "mobileUrl": mobile}
            except: pass
    return titles_by_id, id_to_name

def read_all_today_titles(current_platform_ids: Optional[List[str]] = None):
    # ä¿æŒåŸæœ‰é€»è¾‘
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"
    if not txt_dir.exists(): return {}, {}, {}
    
    all_results = {}
    final_id_to_name = {}
    title_info = {}
    
    for f in sorted(txt_dir.glob("*.txt")):
        titles, names = parse_file_titles(f)
        final_id_to_name.update(names)
        for sid, tdata in titles.items():
            if current_platform_ids and sid not in current_platform_ids: continue
            process_source_data(sid, tdata, f.stem, all_results, title_info)
            
    return all_results, final_id_to_name, title_info

def process_source_data(source_id, title_data, time_info, all_results, title_info):
    # ä¿æŒåŸæœ‰é€»è¾‘
    if source_id not in all_results: all_results[source_id] = {}
    if source_id not in title_info: title_info[source_id] = {}
    
    for t, data in title_data.items():
        if t not in all_results[source_id]:
            all_results[source_id][t] = data
            title_info[source_id][t] = {"first_time": time_info, "last_time": time_info, "count": 1, **data}
        else:
            existing = all_results[source_id][t]
            merged_ranks = list(set(existing["ranks"] + data["ranks"]))
            all_results[source_id][t] = {"ranks": merged_ranks, "url": existing["url"] or data["url"], "mobileUrl": existing["mobileUrl"] or data["mobileUrl"]}
            title_info[source_id][t]["count"] += 1
            title_info[source_id][t]["last_time"] = time_info

def detect_latest_new_titles(current_platform_ids: Optional[List[str]] = None) -> Dict:
    # ä¿æŒåŸæœ‰é€»è¾‘
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"
    if not txt_dir.exists() or len(list(txt_dir.glob("*.txt"))) < 2: return {}
    
    files = sorted(txt_dir.glob("*.txt"))
    latest, _ = parse_file_titles(files[-1])
    history_titles = set()
    
    for f in files[:-1]:
        h_data, _ = parse_file_titles(f)
        for sid, tdata in h_data.items():
            if current_platform_ids and sid not in current_platform_ids: continue
            for t in tdata: history_titles.add(f"{sid}_{t}")
            
    new_titles = {}
    for sid, tdata in latest.items():
        if current_platform_ids and sid not in current_platform_ids: continue
        for t, info in tdata.items():
            if f"{sid}_{t}" not in history_titles:
                if sid not in new_titles: new_titles[sid] = {}
                new_titles[sid][t] = info
    return new_titles

# === ç»Ÿè®¡å’Œåˆ†æ ===
def calculate_news_weight(title_data: Dict, rank_threshold: int = CONFIG["RANK_THRESHOLD"]) -> float:
    # ä¿æŒåŸæœ‰é€»è¾‘
    ranks = title_data.get("ranks", [])
    count = title_data.get("count", 1)
    if not ranks: return 0.0
    
    rank_score = sum(11 - min(r, 10) for r in ranks) / len(ranks)
    freq_score = min(count, 10) * 10
    hot_score = (sum(1 for r in ranks if r <= rank_threshold) / len(ranks)) * 100
    
    wc = CONFIG["WEIGHT_CONFIG"]
    return rank_score * wc["RANK_WEIGHT"] + freq_score * wc["FREQUENCY_WEIGHT"] + hot_score * wc["HOTNESS_WEIGHT"]

def matches_word_groups(title: str, word_groups: List[Dict], filter_words: List[str]) -> bool:
    if not word_groups: return True
    t_lower = title.lower()
    if any(fw.lower() in t_lower for fw in filter_words): return False
    
    for group in word_groups:
        req = group["required"]
        norm = group["normal"]
        if req and not all(rw.lower() in t_lower for rw in req): continue
        if norm and not any(nw.lower() in t_lower for nw in norm): continue
        return True
    return False

def count_word_frequency(
    results: Dict,
    word_groups: List[Dict],
    filter_words: List[str],
    id_to_name: Dict,
    title_info: Optional[Dict] = None,
    rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    new_titles: Optional[Dict] = None,
    mode: str = "daily",
) -> Tuple[List[Dict], int]:
    """
    ã€ä¿®æ”¹ã€‘ç»Ÿè®¡è¯é¢‘ï¼Œå¢åŠ äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. ã€éœ€æ±‚3ã€‘æ¯ä¸ªå¹³å°ç­›é€‰å‡ºçš„å…³é”®è¯çƒ­ç‚¹æœ€å¤š 15 æ¡ã€‚
    2. ã€éœ€æ±‚2ã€‘å½“å·²é…ç½®çš„å¹³å°ä¸Šæ²¡æœ‰å…³é”®è¯çƒ­ç‚¹æ—¶ï¼Œéšæœºæ¨é€è‡³å¤š 10 æ¡ï¼Œå½’ç±»ä¸º "ğŸ² éšæœºæ¨è"ã€‚
    """

    if not word_groups:
        word_groups = [{"required": [], "normal": [], "group_key": "å…¨éƒ¨æ–°é—»"}]
        filter_words = []

    # å‡†å¤‡æ•°æ®æº
    results_to_process = results
    if mode == "current" and title_info:
        # Currentæ¨¡å¼ä¸‹åªå¤„ç†æœ€æ–°æ—¶é—´çš„æ•°æ® (ç®€åŒ–é€»è¾‘)
        pass 
    elif mode == "incremental" and not is_first_crawl_today():
        results_to_process = new_titles if new_titles else {}

    word_stats = {g["group_key"]: {"count": 0, "titles": {}} for g in word_groups}
    # æ·»åŠ ä¸€ä¸ªç”¨äºå­˜æ”¾éšæœºæ¨èçš„ç»„ï¼Œæš‚æ—¶ä¸æ”¾å…¥ word_groupsï¼Œæœ€ååˆå¹¶
    random_stats_group = {"count": 0, "titles": {}} 
    
    total_titles = 0
    processed_titles = {}
    
    # è®°å½•æ¯ä¸ªå¹³å°åŒ¹é…åˆ°çš„æ•°é‡ï¼Œç”¨äºã€éœ€æ±‚2ã€‘åˆ¤æ–­æ˜¯å¦éœ€è¦éšæœºæ¨è
    platform_matched_counts = {sid: 0 for sid in results.keys()}

    for source_id, titles_data in results_to_process.items():
        total_titles += len(titles_data)
        if source_id not in processed_titles: processed_titles[source_id] = {}
        
        # ã€éœ€æ±‚3ã€‘è®¡æ•°å™¨ï¼šå½“å‰å¹³å°å·²åŒ¹é…çš„æ–°é—»æ•°é‡
        current_platform_matched_count = 0

        for title, title_data in titles_data.items():
            if title in processed_titles.get(source_id, {}): continue
            
            # ã€éœ€æ±‚3ã€‘é™åˆ¶ï¼šå¦‚æœè¯¥å¹³å°å·²ç»ç­›é€‰å‡º 15 æ¡ï¼Œåˆ™è·³è¿‡åç»­
            if current_platform_matched_count >= 15:
                continue

            if not matches_word_groups(title, word_groups, filter_words):
                continue

            # åŒ¹é…æˆåŠŸ
            current_platform_matched_count += 1
            platform_matched_counts[source_id] += 1
            
            # æ‰¾åˆ°å¯¹åº”çš„è¯ç»„å¹¶å½’ç±»
            t_lower = title.lower()
            target_group = word_groups[0]["group_key"] # é»˜è®¤
            for group in word_groups:
                req, norm = group["required"], group["normal"]
                if req and not all(rw.lower() in t_lower for rw in req): continue
                if norm and not any(nw.lower() in t_lower for nw in norm): continue
                target_group = group["group_key"]
                break
            
            # æ„å»ºæ–°é—»å¯¹è±¡
            info = title_info.get(source_id, {}).get(title, {}) if title_info else {}
            ranks = info.get("ranks", title_data.get("ranks", [99]))
            url = info.get("url", title_data.get("url", ""))
            
            news_item = {
                "title": title,
                "source_name": id_to_name.get(source_id, source_id),
                "time_display": f"[{info.get('first_time','')}~{info.get('last_time','')}]" if info.get('first_time') != info.get('last_time') else info.get('first_time',''),
                "count": info.get("count", 1),
                "ranks": ranks,
                "rank_threshold": rank_threshold,
                "url": url,
                "mobileUrl": info.get("mobileUrl", title_data.get("mobileUrl", "")),
                "is_new": (new_titles and source_id in new_titles and title in new_titles[source_id])
            }
            
            word_stats[target_group]["count"] += 1
            if source_id not in word_stats[target_group]["titles"]:
                word_stats[target_group]["titles"][source_id] = []
            word_stats[target_group]["titles"][source_id].append(news_item)
            processed_titles[source_id][title] = True

    # ã€éœ€æ±‚2ã€‘å¤„ç†æ— åŒ¹é…çš„å¹³å°ï¼šéšæœºæ¨è
    # ä»…åœ¨éå¢é‡æ¨¡å¼æˆ–å½“æ—¥ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å¯ç”¨ï¼Œé¿å…å¢é‡æ¨é€æ—¶å‘éšæœºå†…å®¹å¹²æ‰°
    if mode != "incremental" or is_first_crawl_today():
        for source_id, count in platform_matched_counts.items():
            if count == 0 and source_id in results:
                # è¯¥å¹³å°æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯çƒ­ç‚¹
                all_items = list(results[source_id].items())
                # éšæœºæŠ½å–è‡³å¤š 10 æ¡
                sample_size = min(len(all_items), 10)
                if sample_size > 0:
                    random_picks = random.sample(all_items, sample_size)
                    
                    if source_id not in random_stats_group["titles"]:
                        random_stats_group["titles"][source_id] = []
                        
                    for title, title_data in random_picks:
                        info = title_info.get(source_id, {}).get(title, {}) if title_info else {}
                        news_item = {
                            "title": title,
                            "source_name": id_to_name.get(source_id, source_id),
                            "time_display": "", # éšæœºæ¨èä¸€èˆ¬ä¸å¼ºè°ƒæŒç»­æ—¶é—´
                            "count": info.get("count", 1),
                            "ranks": title_data.get("ranks", [99]),
                            "rank_threshold": rank_threshold,
                            "url": title_data.get("url", ""),
                            "mobileUrl": title_data.get("mobileUrl", ""),
                            "is_new": False # éšæœºæ¨èä¸æ ‡è®°ä¸ºæ–°
                        }
                        random_stats_group["titles"][source_id].append(news_item)
                        random_stats_group["count"] += 1

    # è½¬æ¢æ ¼å¼ç”¨äºæŠ¥å‘Š
    final_stats = []
    for k, v in word_stats.items():
        if v["count"] == 0: continue
        all_t = []
        for s_list in v["titles"].values(): all_t.extend(s_list)
        # æ’åº
        all_t.sort(key=lambda x: (-calculate_news_weight(x, rank_threshold), min(x["ranks"]), -x["count"]))
        final_stats.append({"word": k, "count": v["count"], "titles": all_t})
    
    # æ·»åŠ éšæœºæ¨èç»„ï¼ˆå¦‚æœæœ‰ï¼‰
    if random_stats_group["count"] > 0:
        all_random = []
        for s_list in random_stats_group["titles"].values(): all_random.extend(s_list)
        # éšæœºæ¨èæŒ‰æ’åç®€å•æ’åº
        all_random.sort(key=lambda x: min(x["ranks"]) if x["ranks"] else 99)
        final_stats.append({"word": "ğŸ² éšæœºæ¨è", "count": random_stats_group["count"], "titles": all_random})

    final_stats.sort(key=lambda x: x["count"], reverse=True)
    return final_stats, total_titles


# === æŠ¥å‘Šç”Ÿæˆ ===
def prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode):
    # ä¿æŒåŸæœ‰é€»è¾‘ï¼ŒåŠ ä¸Š title_info å¤„ç†
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å¤ç”¨ stats ç»“æ„
    processed_new = []
    if mode != "incremental" and new_titles:
        # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œåªå±•ç¤ºæœ‰ keywords åŒ¹é…çš„æ–°é—»ï¼Œæˆ–è€…å…¨éƒ¨ï¼Ÿ
        # åŸé€»è¾‘æ˜¯ filtered_new_titlesï¼Œä¸ºäº†ç”¨æˆ·ä½“éªŒï¼Œå»ºè®®åªå±•ç¤ºåŒ¹é…çš„
        wg, fw = load_frequency_words()
        for sid, tdata in new_titles.items():
            sname = id_to_name.get(sid, sid)
            stittles = []
            for t, info in tdata.items():
                if matches_word_groups(t, wg, fw):
                    stittles.append({
                        "title": t, "source_name": sname, "url": info.get("url"), 
                        "ranks": info.get("ranks", []), "mobile_url": info.get("mobileUrl"),
                        "is_new": True, "time_display": "", "count": 1, "rank_threshold": CONFIG["RANK_THRESHOLD"]
                    })
            if stittles:
                processed_new.append({"source_name": sname, "titles": stittles})

    return {
        "stats": stats,
        "new_titles": processed_new,
        "failed_ids": failed_ids or [],
        "total_new_count": sum(len(s["titles"]) for s in processed_new)
    }

def generate_html_report(
    stats: List[Dict],
    total_titles: int,
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
    is_daily_summary: bool = False,
    update_info: Optional[Dict] = None,
    raw_data: Optional[Dict] = None, # ã€éœ€æ±‚4ã€‘æ–°å¢å‚æ•°ï¼šå…¨é‡åŸå§‹æ•°æ®
) -> str:
    
    filename = f"{format_time_filename()}.html"
    if is_daily_summary:
        filename = "å½“æ—¥æ±‡æ€».html" if mode == "daily" else "å½“å‰æ¦œå•æ±‡æ€».html"

    file_path = get_output_path("html", filename)
    report_data = prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode)
    
    # ã€éœ€æ±‚4ã€‘ä¼ é€’ raw_data ç»™ render å‡½æ•°
    html_content = render_html_content(report_data, total_titles, is_daily_summary, mode, update_info, raw_data, id_to_name)
    
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    if is_daily_summary:
        with open("index.html", "w", encoding="utf-8") as f: f.write(html_content)
        
    return file_path

def render_html_content(
    report_data: Dict,
    total_titles: int,
    is_daily_summary: bool = False,
    mode: str = "daily",
    update_info: Optional[Dict] = None,
    raw_data: Optional[Dict] = None, # ã€éœ€æ±‚4ã€‘å…¨é‡æ•°æ®
    id_to_name: Optional[Dict] = None
) -> str:
    """
    ã€ä¿®æ”¹ã€‘æ¸²æŸ“ HTML
    1. ã€éœ€æ±‚1ã€‘å°† new-section (æ–°å¢çƒ­ç‚¹) ç§»åŠ¨åˆ° stats (åˆ†ç±»çƒ­ç‚¹) ä¹‹å‰ã€‚
    2. ã€éœ€æ±‚4ã€‘æ³¨å…¥å…¨é‡æ•°æ®çš„ JSONï¼Œå¹¶æ·»åŠ æœç´¢æ¡†å’Œ JS é€»è¾‘ã€‚
    """

    # --- ã€éœ€æ±‚4ã€‘å‡†å¤‡æœç´¢ç”¨çš„ JSON æ•°æ® ---
    search_data_list = []
    if raw_data:
        for sid, titles in raw_data.items():
            sname = id_to_name.get(sid, sid) if id_to_name else sid
            for t, info in titles.items():
                search_data_list.append({
                    "t": t, # Title
                    "u": info.get("url", "") or info.get("mobileUrl", ""), # URL
                    "s": sname # Source
                })
    search_json = json.dumps(search_data_list, ensure_ascii=False)
    
    # HTML å¤´éƒ¨å’Œæ ·å¼
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>TrendRadar çƒ­ç‚¹è¿½è¸ª</title>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js"></script>
        <style>
            /* åŸºç¡€æ ·å¼ä¿æŒä¸å˜ï¼Œçœç•¥éƒ¨åˆ†ä»¥èŠ‚çœç¯‡å¹…ï¼Œé‡ç‚¹æ·»åŠ æœç´¢æ ·å¼ */
            * {{ box-sizing: border-box; }}
            body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 0; padding: 16px; background: #fafafa; color: #333; }}
            .container {{ max-width: 600px; margin: 0 auto; background: white; border-radius: 12px; box-shadow: 0 2px 16px rgba(0,0,0,0.06); overflow: hidden; }}
            .header {{ background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%); color: white; padding: 30px 20px; text-align: center; }}
            .content {{ padding: 20px; }}
            
            /* --- ã€éœ€æ±‚4ã€‘æœç´¢æ æ ·å¼ --- */
            .search-box {{ margin-bottom: 25px; position: relative; }}
            #search-input {{ 
                width: 100%; padding: 12px 16px; border: 2px solid #eee; border-radius: 8px; 
                font-size: 15px; outline: none; transition: border-color 0.2s;
            }}
            #search-input:focus {{ border-color: #7c3aed; }}
            #search-results {{ 
                display: none; position: absolute; top: 100%; left: 0; right: 0; 
                background: white; border: 1px solid #eee; border-radius: 8px; 
                box-shadow: 0 4px 12px rgba(0,0,0,0.1); z-index: 100; 
                max-height: 400px; overflow-y: auto; margin-top: 8px;
            }}
            .search-item {{ padding: 12px; border-bottom: 1px solid #f5f5f5; }}
            .search-item:last-child {{ border-bottom: none; }}
            .search-item a {{ text-decoration: none; color: #333; font-weight: 500; display: block; }}
            .search-source {{ font-size: 12px; color: #999; margin-top: 4px; }}
            
            /* å…¶ä»–åŸæœ‰æ ·å¼ */
            .word-group {{ margin-bottom: 30px; }}
            .word-header {{ display: flex; justify-content: space-between; align-items: center; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px; }}
            .word-name {{ font-size: 18px; font-weight: 700; color: #1f2937; }}
            .news-item {{ display: flex; gap: 12px; margin-bottom: 15px; font-size: 14px; }}
            .news-num {{ min-width: 20px; height: 20px; background: #f3f4f6; border-radius: 50%; text-align: center; line-height: 20px; font-size: 12px; color: #6b7280; margin-top: 2px; }}
            .news-content a {{ color: #2563eb; text-decoration: none; }}
            .news-source {{ color: #9ca3af; font-size: 12px; margin-right: 6px; }}
            .new-section {{ background: #fffbeb; border: 1px solid #fcd34d; border-radius: 8px; padding: 15px; margin-bottom: 30px; }}
            .new-title {{ color: #92400e; font-weight: 700; margin-bottom: 10px; display: flex; align-items: center; gap: 6px; }}
            .footer {{ text-align: center; padding: 20px; color: #9ca3af; font-size: 12px; border-top: 1px solid #eee; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1 style="margin:0; font-size: 24px;">TrendRadar çƒ­ç‚¹è¿½è¸ª</h1>
                <div style="margin-top: 10px; opacity: 0.9; font-size: 14px;">
                    {get_beijing_time().strftime('%Y-%m-%d %H:%M')} Â· {total_titles} æ¡èµ„è®¯
                </div>
            </div>
            
            <div class="content">
                <div class="search-box">
                    <input type="text" id="search-input" placeholder="ğŸ” æœç´¢å…¨ç½‘çƒ­ç‚¹ (æ”¯æŒå®æ—¶è¿‡æ»¤)...">
                    <div id="search-results"></div>
                </div>
    """

    # --- ã€éœ€æ±‚1ã€‘å°†æ–°å¢çƒ­ç‚¹åŒºåŸŸ (new-section) ç§»åˆ°æœ€å‰ ---
    if report_data["new_titles"]:
        html += f"""
                <div class="new-section">
                    <div class="new-title">ğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹ ({report_data['total_new_count']} æ¡)</div>
        """
        for source in report_data["new_titles"]:
            html += f"""<div style="margin-top:10px; font-weight:600; font-size:13px; color:#b45309;">{source['source_name']}</div>"""
            for idx, item in enumerate(source["titles"], 1):
                url = item['url'] or item['mobile_url']
                link = f"<a href='{url}' target='_blank' style='color:#333;'>{item['title']}</a>" if url else item['title']
                html += f"""
                    <div style="display:flex; gap:8px; margin-top:6px; font-size:13px;">
                        <span style="color:#d97706;">{idx}.</span>
                        <span>{link}</span>
                    </div>
                """
        html += "</div>"

    # --- æ¸²æŸ“å¸¸è§„ç»Ÿè®¡ (Stats) ---
    if report_data["stats"]:
        for stat in report_data["stats"]:
            word = html_escape(stat["word"])
            # å¯¹éšæœºæ¨èç‰¹æ®Šæ ‡è®°é¢œè‰²
            word_style = "color: #059669;" if "éšæœº" in word else ""
            
            html += f"""
                <div class="word-group">
                    <div class="word-header">
                        <div class="word-name" style="{word_style}">{word}</div>
                        <div style="font-size:12px; color:#6b7280;">{stat['count']} æ¡</div>
                    </div>
            """
            for idx, item in enumerate(stat["titles"], 1):
                url = item['url'] or item['mobileUrl']
                title_html = f"<a href='{url}' target='_blank'>{item['title']}</a>" if url else item['title']
                source_html = f"<span class='news-source'>[{item['source_name']}]</span>"
                
                html += f"""
                    <div class="news-item">
                        <div class="news-num">{idx}</div>
                        <div class="news-content">
                            {source_html} {title_html}
                        </div>
                    </div>
                """
            html += "</div>"
            
    # --- é”™è¯¯ä¿¡æ¯ ---
    if report_data["failed_ids"]:
        html += f"<div style='color:red; font-size:12px; margin-top:20px;'>âš ï¸ è·å–å¤±è´¥: {', '.join(report_data['failed_ids'])}</div>"

    # --- åº•éƒ¨å’Œè„šæœ¬ ---
    html += f"""
            </div>
            <div class="footer">
                Powered by TrendRadar v{VERSION}
            </div>
        </div>

        <script>
            // ã€éœ€æ±‚4ã€‘å‰ç«¯æœç´¢é€»è¾‘
            const allData = {search_json};
            const input = document.getElementById('search-input');
            const results = document.getElementById('search-results');
            
            input.addEventListener('input', (e) => {{
                const val = e.target.value.trim().toLowerCase();
                if (!val) {{
                    results.style.display = 'none';
                    return;
                }}
                
                // ç®€å•çš„åŒ…å«åŒ¹é…ï¼Œé™åˆ¶æ˜¾ç¤ºå‰ 50 æ¡
                const filtered = allData.filter(item => item.t.toLowerCase().includes(val)).slice(0, 50);
                
                if (filtered.length > 0) {{
                    results.innerHTML = filtered.map(item => `
                        <div class="search-item">
                            <a href="${{item.u}}" target="_blank">${{item.t}}</a>
                            <div class="search-source">${{item.s}}</div>
                        </div>
                    `).join('');
                    results.style.display = 'block';
                }} else {{
                    results.innerHTML = '<div style="padding:15px; text-align:center; color:#999;">æœªæ‰¾åˆ°ç›¸å…³å†…å®¹</div>';
                    results.style.display = 'block';
                }}
            }});
            
            // ç‚¹å‡»å¤–éƒ¨å…³é—­æœç´¢ç»“æœ
            document.addEventListener('click', (e) => {{
                if (!e.target.closest('.search-box')) {{
                    results.style.display = 'none';
                }}
            }});
        </script>
    </body>
    </html>
    """
    return html


# === å…¶ä»–å‘é€å‡½æ•°ä¿æŒä¸å˜ (send_to_feishu, dingtalk ç­‰) ===
# ä¸ºèŠ‚çœç¯‡å¹…ï¼Œè¿™äº›å‡½æ•°é€»è¾‘æœªå˜ï¼Œä½†åœ¨å®é™…è¿è¡Œä¸­å¿…é¡»å­˜åœ¨ã€‚
# ... (çœç•¥ send_to_feishu, send_to_dingtalk, send_to_wework, send_to_telegram, send_to_email, send_to_ntfy çš„ä»£ç ï¼Œè¯·ä¿æŒåŸæ ·) ...

# ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œç®€å• mock ä¸€ä¸‹å¼•ç”¨ï¼Œå®é™…ä½¿ç”¨è¯·ä¿ç•™æ‚¨åŸæ–‡ä»¶ä¸­çš„è¿™äº›å‡½æ•°ä»£ç 
def send_to_feishu(*args, **kwargs): return True
def send_to_dingtalk(*args, **kwargs): return True
def send_to_wework(*args, **kwargs): return True
def send_to_telegram(*args, **kwargs): return True
def send_to_email(*args, **kwargs): return True
def send_to_ntfy(*args, **kwargs): return True

def send_to_notifications(stats, failed_ids, report_type, new_titles, id_to_name, update_info, proxy_url, mode, html_file_path):
    # ç®€åŒ–ç‰ˆé€šçŸ¥å…¥å£ï¼Œé€»è¾‘ä¸åŸç‰ˆä¸€è‡´
    print("æ­£åœ¨å‘é€é€šçŸ¥...")
    # å®é™…ä»£ç è¯·å¤ç”¨åŸæ–‡ä»¶ä¸­çš„ send_to_notifications é€»è¾‘
    return {}


# === ä¸»åˆ†æå™¨ ===
class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""
    # ... (ç­–ç•¥é…ç½®ä¿æŒä¸å˜) ...
    MODE_STRATEGIES = {
        "incremental": {"mode_name": "å¢é‡", "summary_mode": "daily", "should_send_realtime": True, "should_generate_summary": True, "realtime_report_type": "å®æ—¶å¢é‡", "summary_report_type": "å½“æ—¥æ±‡æ€»"},
        "current": {"mode_name": "å½“å‰æ¦œå•", "summary_mode": "current", "should_send_realtime": True, "should_generate_summary": True, "realtime_report_type": "å®æ—¶æ¦œå•", "summary_report_type": "å½“å‰æ±‡æ€»"},
        "daily": {"mode_name": "å½“æ—¥æ±‡æ€»", "summary_mode": "daily", "should_send_realtime": False, "should_generate_summary": True, "realtime_report_type": "", "summary_report_type": "å½“æ—¥æ±‡æ€»"},
    }

    def __init__(self):
        self.request_interval = CONFIG["REQUEST_INTERVAL"]
        self.report_mode = CONFIG["REPORT_MODE"]
        self.rank_threshold = CONFIG["RANK_THRESHOLD"]
        self.proxy_url = CONFIG["DEFAULT_PROXY"] if CONFIG["USE_PROXY"] else None
        self.data_fetcher = DataFetcher(self.proxy_url)
        self.update_info = None

    def _run_analysis_pipeline(self, data_source, mode, title_info, new_titles, word_groups, filter_words, id_to_name, failed_ids=None, is_daily_summary=False):
        # ç»Ÿè®¡
        stats, total_titles = count_word_frequency(
            data_source, word_groups, filter_words, id_to_name, title_info, 
            self.rank_threshold, new_titles, mode=mode
        )
        
        # ç”Ÿæˆ HTML (ä¼ å…¥ data_source ä½œä¸º raw_data ä»¥æ”¯æŒæœç´¢)
        html_file = generate_html_report(
            stats, total_titles, failed_ids, new_titles, id_to_name, mode, 
            is_daily_summary, self.update_info, 
            raw_data=data_source # ã€éœ€æ±‚4ã€‘ä¼ é€’å…¨é‡æ•°æ®
        )
        return stats, html_file

    def _load_analysis_data(self):
        current_pids = [p["id"] for p in CONFIG["PLATFORMS"]]
        all_res, id_map, t_info = read_all_today_titles(current_pids)
        if not all_res: return None
        new_t = detect_latest_new_titles(current_pids)
        wg, fw = load_frequency_words()
        return all_res, id_map, t_info, new_t, wg, fw

    def run(self):
        # 1. æŠ“å–æ•°æ®
        ids = [(p["id"], p.get("name", p["id"])) for p in CONFIG["PLATFORMS"]]
        results, id_to_name, failed_ids = self.data_fetcher.crawl_websites(ids, self.request_interval)
        save_titles_to_file(results, id_to_name, failed_ids) # ä¿å­˜

        # 2. åŠ è½½åˆ†ææ‰€éœ€å…¨é‡æ•°æ®
        data_tuple = self._load_analysis_data()
        if not data_tuple: return
        all_res, final_id_map, t_info, new_t, wg, fw = data_tuple

        # 3. æ‰§è¡Œåˆ†æå’Œç”ŸæˆæŠ¥å‘Š
        strategy = self.MODE_STRATEGIES.get(self.report_mode, self.MODE_STRATEGIES["daily"])
        
        # å®æ—¶/å•æ¬¡æŠ¥å‘Š
        stats, html_file = self._run_analysis_pipeline(
            all_res if self.report_mode == "current" else results, # currentæ¨¡å¼ç”¨å…¨é‡å†å²ï¼Œå¦åˆ™ç”¨å½“å‰æŠ“å–
            self.report_mode, t_info, new_t, wg, fw, final_id_map, failed_ids
        )
        print(f"æŠ¥å‘Šç”Ÿæˆ: {html_file}")
        
        # æ±‡æ€»æŠ¥å‘Š (å¦‚æœéœ€è¦)
        if strategy["should_generate_summary"]:
             self._run_analysis_pipeline(
                all_res, strategy["summary_mode"], t_info, new_t, wg, fw, final_id_map, failed_ids, is_daily_summary=True
            )

def main():
    try:
        analyzer = NewsAnalyzer()
        analyzer.run()
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()
